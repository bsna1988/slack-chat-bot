<p><span>Following investigation avenues might be helpful when investigating a suspected  memory leak.</span></p><h2 id="InvestigatingMemoryLeaks-Identifyaminimalpipelinethatreliablyreproducestheleak"><strong>Identify a minimal pipeline that reliably reproduces the leak </strong></h2><ul><li><span>Identify a pipeline that reliably reproduces the  leak. Use runner memory monitoring capabilities to confirm the increase in memory consumption overtime. </span><ul><li><span>For example, on Linux, one can use the `top` utility that can report memory allocated for a particular  process and available memory. Note that Linux might allocate <a class="external-link" href="https://www.linuxatemyram.com/" rel="nofollow">significant amounts of RAM for disk caches</a></span><span>, which might appear as if the system were low on ram.</span></li></ul></li><ul><li><span>Dataflow UI has JOB METRICS -&gt; Memory Utilization page that can provide this information. </span></li><li><span>When executing on the local runner, memory consumption via tools provided for local OS. </span></li></ul><li>If you can run the pipeline with the `--runner=DirectRunner`  option, you might be able to make the leak more pronounced by making every bundle run multiple times using `--direct_runner_bundle_repeat` pipeline option.</li><li><span>If the Direct runner cannot be used to repro, try to  reproduce the leak when the pipeline runs only on 1 worker. It might also help to limit the execution only to a single process on the worker. DataflowRunner users could limit execution to 1 process on 1 worker by specifying: `<code>--num_workers=1 --max_num_workers=1 --experiments=no_use_multiple_sdk_containers`</code> , and further reduce  parallelism if necessary using `--number_of_worker_harness_threads=1`. </span></li></ul><h2 id="InvestigatingMemoryLeaks-DetermineiftheleakisonPythonHeaporaleakinnativeallocations"><strong>Determine if the leak is on Python Heap or a leak in native allocations</strong></h2><p><span>Memory usage growth can happen if a Python process accumulates references to objects on the heap that are no longer being used. Because references are kept, these objects are not  garbage-collected. In this case, the objects will be visible to the Python garbage collector.  </span></p><p><span>There are couple of ways how Python heap could be inspected:</span></p><ul><li>If you set the  ` --experiments=enable_heap_dump` option, heap dumps will be appended to the SDK status responses, which SDK can provide to the runner. Dataflow workers serve the SDK status page on `localhost:8081/sdk_status`, and can be queried via: <strong>gcloud compute ssh --zone &quot;xx-somezone-z&quot; &quot;some-dataflow-gce-worker-01300848-wqox-harness-bvf7&quot; --project &quot;some-project-id&quot; --command &quot;curl localhost:8081/sdk_status&quot; .</strong></li><li><span>The per-workitem heap profiling options  </span><a class="external-link" href="https://github.com/apache/beam/blob/3172736aaa3fb871fc4d7462e6057d971f3b151f/sdks/python/apache_beam/options/pipeline_options.py#L1280-L1293" rel="nofollow"><span>https://github.com/apache/beam/blob/3172736aaa3fb871fc4d7462e6057d971f3b151f/sdks/python/apache_beam/options/pipeline_options.py#L1280-L1293</span></a><span> could be used to inspect the objects that are left in the heap after a bundle execution. </span></li></ul><p><span>Memory leak can happen when C/C++ memory allocations are not released. This leak could be caused by Python extensions used in user code, Beam SDK or its dependencies, or (unlikely but possible) by the Python interpreter itself. Such leaks might not be visible when inspecting objects that live in the Python interpreter heap, but might  be visible when inspecting allocations performed by the Python interpreter process. It may require  a memory profiler that tracks native memory allocations (see below), or substituting a default memory allocator to a custom allocator such as tcmalloc, that can help analyze the heap dump for the entire process (example: <a class="external-link" href="https://github.com/apache/beam/issues/28246#issuecomment-1918087115" rel="nofollow">https://github.com/apache/beam/issues/28246#issuecomment-1918087115</a>).      </span><strong> </strong></p><h2 id="InvestigatingMemoryLeaks-Identifywhetheryouhaveamemoryleakorhighpeakmemoryusage"><strong>Identify whether you have a memory leak or high peak memory usage</strong></h2><p>See: <a class="external-link" href="https://cloud.google.com/dataflow/docs/guides/troubleshoot-oom" rel="nofollow">https://cloud.google.com/dataflow/docs/guides/troubleshoot-oom</a> for discussion of how to evaluate peak memory usage. If the pipeline  suffers from memory fragmentation due to inefficient allocations, you might be able to reduce memory footprint by switching the default memory allocator to a different one, such as jemalloc or tcmalloc. <span style="color: rgb(31,35,40);">Substituting the allocator can be done in a<span> </span></span><a class="external-link" href="https://beam.apache.org/documentation/runtime/environments/" rel="nofollow" style="text-decoration: underline;">custom container</a>.</p><p style="">A<span> </span><code class="notranslate rgh-seen--8513125793 rgh-linkified-code rgh-seen--10237993728">Dockerfile</code><span> </span>for a custom container that substitutes memory allocator might look like the following:</p><pre class="notranslate"><code class="notranslate">FROM<span> </span>apache/beam_python3.10_sdk:2.53.0
RUN<span> </span>apt<span> </span>update<span> </span>;<span> </span>apt<span> </span>install<span> </span>-y<span> libjemalloc2</span>
#<span> </span>Note:<span> </span>this<span> </span>enables<span> </span>jemalloc<span> </span>globally<span> </span>for<span> </span>all<span> </span>applications<span> </span>running<span> </span>in<span> </span>the<span> </span>container<span> </span>
ENV<span> </span>LD_PRELOAD<span> </span>/usr/lib/x86_64-linux-gnu/<a class="external-link" href="http://libjemalloc.so" rel="nofollow">libjemalloc.so</a>.2
</code></pre><h2 id="InvestigatingMemoryLeaks-Considerusinganexternalprofiler"><strong>Consider using an external profiler</strong></h2><p><span>Using an off-the-shelf memory profiler, such as memray (see: <a class="external-link" href="https://pypi.org/project/memray/" rel="nofollow">https://pypi.org/project/memray/</a>),  can be effective to identify leaks and memory usage patterns, but requires additional instrumentation. </span></p><p><span>Profiling Beam Python pipeline is most effective when the Python program that leaks memory (such as Python SDK harness), is launched by the profiler as opposed to attaching the profiler at runtime, after the process has already started. If the leak can be reproduced in Python direct runner this is straightforward, (example: </span><a class="external-link" href="https://github.com/apache/beam/issues/28246#issuecomment-1918120673" rel="nofollow"><span>https://github.com/apache/beam/issues/28246#issuecomment-1918120673</span></a><span> ). If however the leak is only reproduced in a running pipeline, starting SDK harness from the profiler requires changing the code that launches the SDK harness, such as the Beam SDK harness container entrypoint</span><span>, example: </span><a class="external-link" href="https://github.com/apache/beam/pull/30151/files" rel="nofollow"><span>https://github.com/apache/beam/pull/30151/files</span></a><span>. </span></p><p><span>Such modification requires providing a custom container (example: </span><a class="external-link" href="https://github.com/apache/beam/issues/28246#issuecomment-1918101657" rel="nofollow"><span>https://github.com/apache/beam/issues/28246#issuecomment-1918101657</span></a><span>) </span></p><p><span>It is best if the profiler can collect and output memory allocation statistics while the process is still running. Some tools only output the collected data after the process under investigation  terminates, in which case collecting and accessing profiles from workers might be  more problematic. </span><span>Retrieving profiles may require connecting to the worker and fetching profiles from the running SDK container (example: <a class="external-link" href="https://github.com/apache/beam/issues/28246#issuecomment-1918114643" rel="nofollow">https://github.com/apache/beam/issues/28246#issuecomment-1918114643</a>),  if the profiler stores the collected output on the container instance. </span></p><p><span>Analyzing the profile and creating reports from the collected profile needs to happen in the environment where the profiled binary runs, and since profiler might need to access symbols from shared libraries used by the profiled binary. Therefore, reports should be generated in the same running container, or in a container created from the same image. </span><span>Once reports have been generated, they can be extracted and further inspected on a standalone machine. </span></p><p><span>Memray has many different reporters that can analyze the profiling data.  It may be worth trying several reporters as depending on the source of the leak, some might work better than others. Memray also supports tracking allocations by native code, and profiles can be collected and analyzed while the process under investigation is still running.</span></p><p><span>The </span><a class="external-link" href="https://github.com/apache/beam/issues/20298" rel="nofollow"><span>https://github.com/apache/beam/issues/20298</span></a> issue<span>  tracks future improvements to make memory profiling easier. </span></p>