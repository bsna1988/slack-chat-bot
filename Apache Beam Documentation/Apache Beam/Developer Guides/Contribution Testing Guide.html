<p><div class="toc-macro client-side-toc-macro  conf-macro output-block" data-hasbody="false" data-headerelements="H1,H2,H3,H4,H5,H6,H7" data-macro-name="toc"> </div></p><h1 id="ContributionTestingGuide-BeamTesting">Beam Testing</h1><p>This document outlines how to write tests, which tests are appropriate where, and when tests are run, with some additional information about the testing systems at the bottom.</p><h2 id="ContributionTestingGuide-TestingScenarios">Testing Scenarios</h2><p>Ideally, all available tests should be run against a pull request (PR) before it’s allowed to be committed to Beam’s <a class="external-link" href="https://github.com/apache/beam" rel="nofollow">Github</a> repo. This is not possible, however, due to a combination of time and resource constraints. Running all tests for each PR would take hours or even days using available resources, which would slow down development considerably.</p><p>Thus tests are split into <em>pre-commit</em> and <em>post-commit</em> suites. Pre-commit is fast, while post-commit is comprehensive. As their names imply, pre-commit tests are run on each PR before it is committed, while post-commits run periodically against the master branch (i.e. on already committed PRs).</p><p>Beam uses <a class="external-link" href="https://ci-beam.apache.org/" rel="nofollow">Jenkins</a> to run pre-commit and post-commit tests.</p><h3 id="ContributionTestingGuide-Pre-commit">Pre-commit</h3><p>The pre-commit test suite verifies correctness via two testing tools: unit tests and end-to-end (E2E) tests. Unit tests ensure correctness at a basic level, while WordCount E2E tests are run againsts each supported SDK / runner combination as a smoke test, to verify that a basic level of functionality exists.</p><p>This combination of tests hits the appropriate tradeoff between a desire for short (ideally &lt;30m) pre-commit times and a desire to verify that PRs going into Beam function in the way in which they are intended.</p><p>Pre-commit jobs are kicked off when a contributor makes a PR against the <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge">apache/beam</code></strong></span> repository. Job statuses are displayed at the bottom of the PR page. Clicking on “Details” will open the status page in the selected tool; there, you can view test status and output.</p><h3 id="ContributionTestingGuide-Post-commit">Post-commit</h3><p>Running in post-commit removes as stringent of a time constraint, which gives us the ability to do some more comprehensive testing. In post-commit we have a test suite running the ValidatesRunner tests against each supported runner, and another for running the full set of E2E tests against each runner. Currently-supported runners are Dataflow, Flink, Spark, and Gearpump, with others soon to follow. Work is ongoing to enable Flink, Spark, and Gearpump in the E2E framework, with full support targeted for end of August 2016. Post-commit tests run periodically, with timing defined in their Jenkins configurations.</p><p>Adding new post-commit E2E tests is generally as easy as adding a *IT.java file to the repository - Failsafe will notice it and run it - but if you want to do more interesting things, take a look at <a class="external-link" href="https://github.com/apache/beam/blob/master/examples/java/src/test/java/org/apache/beam/examples/WordCountIT.java" rel="nofollow">WordCountIT.java</a>.</p><p>Post-commit test results can be found in <a class="external-link" href="https://ci-beam.apache.org/" rel="nofollow">Jenkins</a>.</p><h2 id="ContributionTestingGuide-TestingTypes">Testing Types</h2><h3 id="ContributionTestingGuide-Unit">Unit</h3><p>Unit tests are, in Beam as everywhere else, the first line of defense in ensuring software correctness. As all of the contributors to Beam understand the importance of testing, Beam has a robust set of unit tests, as well as testing coverage measurement tools, which protect the codebase from simple to moderate breakages. Beam Java unit tests are written in JUnit.</p><h4 id="ContributionTestingGuide-HowtorunPythonunittests">How to run Python unit tests</h4><p>Python tests are written using the standard Python unittest library. To run all unit tests, execute the following command in the <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge">sdks/python</code></strong></span> subdirectory</p><div class="code panel pdl conf-macro output-block" data-hasbody="true" data-macro-name="code" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: bash; gutter: false; theme: Default" data-theme="Default">$ python setup.py test [-s apache_beam.package.module.TestClass.test_method]</pre>
</div></div><pre class="highlight"><code>
</code></pre><p>We also provide a <a class="external-link" href="https://tox.readthedocs.io/en/latest/" rel="nofollow">tox</a> configuration in that same directory to run all the tests, including lint, cleanly in all desired configurations.</p><h4 id="ContributionTestingGuide-HowtorunJavaNeedsRunnertests">How to run Java NeedsRunner tests</h4><p>NeedsRunner is a category of tests that require a Beam runner. To run NeedsRunner tests:</p><div class="code panel pdl conf-macro output-block" data-hasbody="true" data-macro-name="code" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: bash; gutter: false; theme: Default" data-theme="Default">$ ./gradlew runners:direct-java:needsRunnerTests</pre>
</div></div><p>To run a single NeedsRunner test use the <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge">test</code></strong></span> property, e.g.</p><div class="code panel pdl conf-macro output-block" data-hasbody="true" data-macro-name="code" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: bash; gutter: false; theme: Default" data-theme="Default">$ ./gradlew runners:direct-java:needsRunnerTests --tests org.apache.beam.sdk.transforms.MapElementsTest.testMapBasicProcessFunction</pre>
</div></div><p>will run the <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge">MapElementsTest.testMapBasicProcessFunction()</code></strong></span> test.</p><p>NeedsRunner tests in modules that are not required to build runners (e.g. <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge">sdks/java/io/google-cloud-platform</code></strong></span>) can be executed with the <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge">gradle test</code></strong></span> command:</p><div class="code panel pdl conf-macro output-block" data-hasbody="true" data-macro-name="code" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: bash; gutter: false; theme: Default" data-theme="Default">$ ./gradlew sdks:java:io:google-cloud-platform:test --tests org.apache.beam.sdk.io.gcp.spanner.SpannerIOWriteTest</pre>
</div></div><h3 id="ContributionTestingGuide-ValidatesRunner">ValidatesRunner</h3><p>ValidatesRunner tests contain components of both component and end-to-end tests. They fulfill the typical purpose of a component test - they are meant to test a well-scoped piece of Beam functionality or the interactions between two such pieces and can be run in a component-test-type fashion against the DirectRunner. Additionally, they are built with the ability to run in an end-to-end fashion against a runner, allowing them to verify not only core Beam functionality, but runner functionality as well. They are more lightweight than a traditional end-to-end test and, because of their well-scoped nature, provide good signal as to what exactly is working or broken against a particular runner.</p><h3 id="ContributionTestingGuide-E2E">E2E</h3><p>End-to-End tests are meant to verify at the very highest level that the Beam codebase is working as intended. Because they are implemented as a thin wrapper around existing pipelines, they can be used to prove that the core Beam functionality is available. They will be used to verify runner correctness, but they can also be used for IO connectors and other core functionality.</p><h2 id="ContributionTestingGuide-TestingSystems">Testing Systems</h2><h3 id="ContributionTestingGuide-E2ETestingFramework">E2E Testing Framework</h3><p>The Beam end-to-end testing framework is a framework designed in a runner-agnostic fashion to exercise the entire lifecycle of a Beam pipeline. We run a pipeline as a user would and allow it to run to completion in the same way, verifying after completion that it behaved how we expected. Using pipelines from the Beam examples, or custom-built pipelines, the framework will provide hooks during several pipeline lifecycle events, e.g., pipeline creation, pipeline success, and pipeline failure, to allow verification of pipeline state.</p><p>The E2E testing framework is currently built to execute the tests on Jenkins, invoked via Gradle tasks. Once it is determined how Python and other future languages will integrate into the overall build/test system (via Gradle or otherwise) we will adjust this. The framework provides a wrapper around actual Beam pipelines, enabling those pipelines to be run in an environment which facilitates verification of pipeline results and details.</p><p>Verifiers include:</p><ul><li>Output verification. Output verifiers ensure that the pipeline has produced the expected output. Current verifiers check text-based output, but future verifiers could support other output such as BigQuery and Datastore.</li><li>Aggregator verification. Aggregator verifiers ensure that the user-defined aggregators present in the pipelines under test finish in the expected state.</li></ul><p>The E2E framework will support running on various different configurations of environments. We currently provide the ability to run against the DirectRunner, against a local Spark instance, a local Flink instance, and against the Google Cloud Dataflow service.</p><h3 id="ContributionTestingGuide-ValidatesRunnerTests">ValidatesRunner Tests</h3><p>ValidatesRunner tests are tests built to use the Beam TestPipeline class, which enables test authors to write simple functionality verification. They are meant to use some of the built-in utilities of the SDK, namely PAssert, to verify that the simple pipelines they run end in the correct state.</p><h3 id="ContributionTestingGuide-EffectiveuseoftheTestPipelineJUnitrule">Effective use of the TestPipeline JUnit rule</h3><p><span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge">TestPipeline</code></strong></span> is JUnit rule designed to facilitate testing pipelines. In combination with <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge">PAssert</code></strong></span>, the two can be used for testing and writing assertions over pipelines. However, in order for these assertions to be effective, the constructed pipeline <strong>must</strong> be run by a pipeline runner. If the pipeline is not run (i.e., executed) then the constructed <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge">PAssert</code></strong></span> statements will not be triggered, and will thus be ineffective.</p><p>To prevent such cases, <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge">TestPipeline</code></strong></span> has some protection mechanisms in place.</p><p><strong>Abandoned node detection (performed automatically)</strong></p><p>Abandoned nodes are <strong><code class="highlighter-rouge">PTransforms</code></strong>, <strong><code class="highlighter-rouge">PAsserts</code></strong> included, that were not executed by the pipeline runner. Abandoned nodes are most likely to occur due to the one of the following scenarios:</p><ol><li>Lack of a <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge">pipeline.run()</code></strong></span> statement at the end of a test.</li><li>Addition of <strong><code class="highlighter-rouge"><span style="color: rgb(255,0,0);">PTransform</span> </code></strong>s after the pipeline has already run.</li></ol><p>Abandoned node detection is <em>automatically enabled</em> when a real pipeline runner (i.e. not a <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge">CrashingRunner</code></strong></span>) and/or a <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge">@NeedsRunner</code></strong></span> / <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge">@ValidatesRunner</code></strong></span> annotation are detected.</p><p>Consider the following test:</p><p><strong>Java</strong></p><div class="code panel pdl conf-macro output-block" data-hasbody="true" data-macro-name="code" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Default" data-theme="Default">// Note the @Rule annotation here
@Rule
public final transient TestPipeline pipeline = TestPipeline.create();

@Test
@Category(NeedsRunner.class)
public void myPipelineTest() throws Exception {

final PCollection&lt;String&gt; pCollection =
  pipeline
    .apply(&quot;Create&quot;, Create.of(WORDS).withCoder(StringUtf8Coder.of()))
    .apply(
        &quot;Map1&quot;,
        MapElements.via(
            new SimpleFunction&lt;String, String&gt;() {

              @Override
              public String apply(final String input) {
                return WHATEVER;
              }
            }));

PAssert.that(pCollection).containsInAnyOrder(WHATEVER);       

/* ERROR: pipeline.run() is missing, PAsserts are ineffective */
}</pre>
</div></div><p><strong>Python</strong></p><div class="code panel pdl conf-macro output-block" data-hasbody="true" data-macro-name="code" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: py; gutter: false; theme: Default" data-theme="Default"># The suggested pattern of using pipelines as targets of with statements
# eliminates the possibility for this kind of error or the need for a
# framework to catch it.

with beam.Pipeline(...) as p:
    [...arbitrary construction...]
    # p.run() is automatically called on successfully exiting the context</pre>
</div></div><p>The <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge">PAssert</code></strong></span> at the end of this test method will not be executed, since <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge">pipeline</code></strong></span> is never run, making this test ineffective. If this test method is run using an actual pipeline runner, an exception will be thrown indicating that there was no <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge">run()</code></strong></span> invocation in the test.</p><p>Exceptions that are thrown prior to executing a pipeline, will fail the test unless handled by an <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge">ExpectedException</code></strong></span> rule.</p><p>Consider the following test:</p><p><strong>Java</strong></p><div class="code panel pdl conf-macro output-block" data-hasbody="true" data-macro-name="code" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Default" data-theme="Default">Java
// Note the @Rule annotation here
@Rule
public final transient TestPipeline pipeline = TestPipeline.create();

@Test
public void testReadingFailsTableDoesNotExist() throws Exception {
  final String table = &quot;TEST-TABLE&quot;;

  BigtableIO.Read read =
      BigtableIO.read()
          .withBigtableOptions(BIGTABLE_OPTIONS)
          .withTableId(table)
          .withBigtableService(service);

  // Exception will be thrown by read.validate() when read is applied.
  thrown.expect(IllegalArgumentException.class);
  thrown.expectMessage(String.format(&quot;Table %s does not exist&quot;, table));

  p.apply(read);
}</pre>
</div></div><p><strong>Python</strong></p><div class="code panel pdl conf-macro output-block" data-hasbody="true" data-macro-name="code" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: py; gutter: false; theme: Default" data-theme="Default"># Use standard Python tools such as unittest's TestCase.assertRaises</pre>
</div></div><p>The application of the <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge" style="letter-spacing: 0.0px;">read</code></strong></span> transform throws an exception, which is then handled by the <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge" style="letter-spacing: 0.0px;">thrown</code></strong></span> <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge" style="letter-spacing: 0.0px;">ExpectedException</code></strong></span> rule. In light of this exception, the fact this test has abandoned nodes (the <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge" style="letter-spacing: 0.0px;">read</code></strong></span> transform) does not play a role since the test fails before the pipeline would have been executed (had there been a <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge" style="letter-spacing: 0.0px;">run()</code></strong></span> statement).</p><p><strong>Auto-add <span style="color: rgb(255,0,0);"><code class="highlighter-rouge">pipeline.run()</code></span> (disabled by default)</strong></p><p>A <strong><span style="color: rgb(255,0,0);"><code class="highlighter-rouge">TestPipeline</code></span></strong> instance can be configured to auto-add a missing <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge">run()</code></strong></span> statement by setting <strong><span style="color: rgb(255,0,0);"><code class="highlighter-rouge">testPipeline.enableAutoRunIfMissing(true/false)</code></span></strong>. If this feature is enabled, no exception will be thrown in case of a missing <span style="color: rgb(255,0,0);"><strong>run()</strong></span> statement, instead, one will be added automatically.</p><h3 id="ContributionTestingGuide-APISurfacetesting">API Surface testing</h3><p>The surface of an API is the set of public classes that are exposed to the outer world. In order to keep the API tight and avoid unnecessarily exposing classes, Beam provides the <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge">ApiSurface</code></strong></span> utility class. Using the <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge">ApiSurface</code></strong></span> class, we can assert the API surface against an expected set of classes.</p><p>Consider the following snippet:</p><p><strong>Java</strong></p><div class="code panel pdl conf-macro output-block" data-hasbody="true" data-macro-name="code" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Default" data-theme="Default">@Test
public void testMyApiSurface() throws Exception {

    final Package thisPackage = getClass().getPackage();
    final ClassLoader thisClassLoader = getClass().getClassLoader();

    final ApiSurface apiSurface =
        ApiSurface.ofPackage(thisPackage, thisClassLoader)
            .pruningPattern(&quot;org[.]apache[.]beam[.].*Test.*&quot;)
            .pruningPattern(&quot;org[.]apache[.]beam[.].*IT&quot;)
            .pruningPattern(&quot;java[.]lang.*&quot;);

    @SuppressWarnings(&quot;unchecked&quot;)
    final Set&lt;Matcher&lt;Class&lt;?&gt;&gt;&gt; allowed =
        ImmutableSet.of(
            classesInPackage(&quot;org.apache.beam.x&quot;),
            classesInPackage(&quot;org.apache.beam.y&quot;),
            classesInPackage(&quot;org.apache.beam.z&quot;),
            Matchers.&lt;Class&lt;?&gt;&gt;equalTo(Other.class));

    assertThat(apiSurface, containsOnlyClassesMatching(allowed));
}</pre>
</div></div><p><strong>Python</strong></p><div class="code panel pdl conf-macro output-block" data-hasbody="true" data-macro-name="code" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: py; gutter: false; theme: Default" data-theme="Default"># Python doesn't have the notion of public vs. private members or type declarations that may accidentally leak.</pre>
</div></div><p>This test will fail if the classes exposed by <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge" style="letter-spacing: 0.0px;">getClass().getPackage()</code></strong></span>, except classes which reside under <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge" style="letter-spacing: 0.0px;">&quot;org[.]apache[.]beam[.].*Test.*&quot;</code></strong></span>,</p><p><span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge">&quot;org[.]apache[.]beam[.].*IT&quot;</code></strong></span> or <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge">&quot;java[.]lang.*&quot;</code></strong></span>, belong to neither of the packages: <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge">org.apache.beam.x</code>, <code class="highlighter-rouge">org.apache.beam.y</code>, <code class="highlighter-rouge">org.apache.beam.z</code></strong></span>, nor equal to <strong><span style="color: rgb(255,0,0);"><code class="highlighter-rouge">Other.class</code></span></strong>.</p><h3 id="ContributionTestingGuide-TestsofCoreApacheBeamOperations">Tests of Core Apache Beam Operations</h3><h4 id="ContributionTestingGuide-Whatarethey?"><span>What are they?</span></h4><p><span>Tests of Core Apache Beam Operations are a set of tests aiming to exercise </span><a class="external-link" href="https://beam.apache.org/documentation/programming-guide/#core-beam-transforms" rel="nofollow"><span>Core Beam transforms</span></a><span> to see how do they behave in stressful conditions. They operate on synthetic data of KV&lt;byte[], byte[]&gt; type that is generated deterministically and can be shaped with different distributions, generation delays and the size of records itself. </span></p><p><span>For example, thanks to the tests users can check things like:</span></p><ul><li><span>Performance impact of hotkeys in input data,</span></li><li><span>Record size impact,</span></li><li><span>State cache capacity,</span></li><li><span>Inter operation overhead,</span></li><li><span>Influence of extensive metrics API usage</span></li></ul><p><span>...and many more.</span></p><p><strong>For a more detailed description of tests and testing scenarios, see the initial proposal: <a class="external-link" href="https://s.apache.org/load-test-basic-operations" rel="nofollow">https://s.apache.org/load-test-basic-operations</a></strong></p><h4 id="ContributionTestingGuide-Runninginstructions"><span>Running instructions</span></h4><p><span>Load test specific parameters:</span></p><div class="table-wrap"><table class="wrapped confluenceTable"><colgroup><col/><col/></colgroup><tbody><tr><td class="confluenceTd"><p><span>Name</span></p></td><td class="confluenceTd"><p><span>Purpose</span></p></td></tr><tr><td class="confluenceTd"><p><span>loadTest.mainClass</span></p></td><td class="confluenceTd"><p><span>The fully qualified name of the testing class</span></p></td></tr><tr><td class="confluenceTd"><p><span>runner</span></p></td><td class="confluenceTd"><p><span>Runner to be used</span></p></td></tr><tr><td class="confluenceTd"><p><span>loadTest.args</span></p></td><td class="confluenceTd"><p><span>Pipeline options for the test</span></p></td></tr></tbody></table></div><h5 id="ContributionTestingGuide-RunningwithDataflowrunner"><span style="color: rgb(94,108,132);font-weight: 600;letter-spacing: 0.0px;">Running with Dataflow runner</span></h5><p><span>Java:</span></p><div class="code panel pdl conf-macro output-block" data-hasbody="true" data-macro-name="code" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Default" data-theme="Default">./gradlew :sdks:java:testing:load-tests:run \
-PloadTest.mainClass=org.apache.beam.sdk.loadtests.GroupByKeyLoadTest \
-Prunner=:runners:google-cloud-dataflow-java \
-PloadTest.args='--project=apache-beam-testing --appName=load_tests_Java_Dataflow_batch_GBK_1 --tempLocation=gs://temp-storage-for-perf-tests/loadtests --publishToBigQuery=true --bigQueryDataset=load_test --bigQueryTable=java_dataflow_batch_GBK_1 --sourceOptions={&quot;numRecords&quot;:200000000,&quot;keySizeBytes&quot;:1,&quot;valueSizeBytes&quot;:9} --fanout=1 --iterations=1 --streaming=false --runner=DataflowRunner' </pre>
</div></div><h5 id="ContributionTestingGuide-RunningwithPortableFlinkRunner"><span><span class="inline-comment-marker" data-ref="bcb5082c-52e0-45b9-8342-5b67fd6088eb">Running with Portable Flink Runner</span></span></h5><p><span>As Flink is not a managed service like Dataflow, you need to setup the cluster before you start the test. We prepared scripts to do that for you using Google Cloud Dataproc. You can see instructions on how to set up a full-blown Flink cluster in </span><a class="external-link" href="https://github.com/apache/beam/blob/master/.test-infra/dataproc/flink_cluster.sh" rel="nofollow"><span>flink_cluster.sh</span></a><span> script.</span></p><p><span>After you set up the cluster, run the tests with the following command:</span></p><p><span>Python + portability:</span></p><div class="code panel pdl conf-macro output-block" data-hasbody="true" data-macro-name="code" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Default" data-theme="Default">./gradlew -PloadTest.args=&quot;
    --publish_to_big_query=true
    --project=apache-beam-testing
    --metrics_dataset=load_test
    --metrics_table=python_flink_batch_GBK_1
    --parallelism=5
    --job_endpoint=localhost:8099
    --environment_config=gcr.io/apache-beam-testing/beam_portability/python2.7_sdk:latest 
    --environment_type=DOCKER
    --iterations=1
    --fanout=1
    --input_options='
      {\&quot;num_records\&quot;: 20000000,
      \&quot;key_size\&quot;: 10,
      \&quot;value_size\&quot;: 90}'
    --runner=PortableRunner&quot; \
-PloadTest.mainClass=apache_beam.testing.load_tests.group_by_key_test \
-Prunner=PortableRunner :sdks:python:apache_beam:testing:load_tests:run</pre>
</div></div><p class="auto-cursor-target"><span>For the purpose of local testing, you can also run the test on an embedded Flink cluster. In this case, you don't need to build and push SDK harness image and setup Flink cluster using Dataproc.<br/></span></p><p class="auto-cursor-target"><span>Before the first run, build a job server:</span></p><div class="code panel pdl conf-macro output-block" data-hasbody="true" data-macro-name="code" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Default" data-theme="Default">./gradlew :runners:flink:1.9:job-server:shadowJar</pre>
</div></div><div class="code panel pdl conf-macro output-block" data-hasbody="true" data-macro-name="code" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Default" data-theme="Default">./gradlew -PloadTest.args=&quot;
    --publish_to_big_query=true
    --project=apache-beam-testing
    --metrics_dataset=load_test
    --metrics_table=python_flink_batch_GBK_1
    --environment_type=LOOPBACK
    --iterations=1
    --fanout=1
    --input_options='
      {\&quot;num_records\&quot;: 20000000,
      \&quot;key_size\&quot;: 10,
      \&quot;value_size\&quot;: 90}'
    --runner=FlinkRunner&quot; \
-PloadTest.mainClass=apache_beam.testing.load_tests.group_by_key_test \
-Prunner=FlinkRunner :sdks:python:apache_beam:testing:load_tests:run</pre>
</div></div><h5 id="ContributionTestingGuide-Jenkinsjobs"><span>Jenkins jobs</span></h5><p><span>There are several Jenkins jobs that are configured to run every 24h. Other than that it is possible to trigger them on demand using Github Pull Request Build Plugin (ghprb). </span></p><p><span>You can find all definitions of the jobs in .test-infra/jenkins directory. Other than that, all the jobs are listed in </span><a class="external-link" href="https://github.com/apache/beam/blob/master/.test-infra/jenkins/README.md" rel="nofollow"><span>README.md</span></a><span> file.</span></p><p><span>When adding a new cron-based Jenkins job, avoid using time slots that are already occupied by other jobs. In order to find which time slots are being used now, take a look at jobs definitions in the <a class="external-link" href="https://github.com/apache/beam/tree/master/.test-infra/jenkins" rel="nofollow">.test-infra/jenkins</a> directory.</span></p><h5 id="ContributionTestingGuide-Dashboards"><span>Dashboards</span></h5><p><span>The load tests are run daily by a set of Jenkins jobs. To be able to assess the current performance of the operations and detect regressions, we created a set of dashboards for the tests. </span></p><p><span>Link for the dashboards: <a class="external-link" href="http://metrics.beam.apache.org" rel="nofollow">http://metrics.beam.apache.org</a>.</span></p><p><br/></p><h2 id="ContributionTestingGuide-Bestpracticesforwritingtests">Best practices for writing tests</h2><p><br/>The following best practices help you to write reliable and maintainable tests.</p><h3 id="ContributionTestingGuide-Aimforonefailurepath">Aim for one failure path</h3><p>An ideal test has one failure path. When you create your tests, minimize the possible reasons for a test failure. A developer can debug a problem more easily when there are fewer failure paths.</p><h3 id="ContributionTestingGuide-Avoidnon-deterministiccode">Avoid non-deterministic code</h3><p>Reliable tests are predictable and deterministic. Tests that contain non-deterministic code are hard to debug and are often flaky. Non-deterministic code includes the use of randomness, time, and multithreading.</p><p>To avoid non-deterministic code, mock the corresponding methods or classes.</p><h3 id="ContributionTestingGuide-Usedescriptivetestnames">Use descriptive test names</h3><p>Helpful test names contain details about your test, such as test parameters and the expected result. Ideally, a developer can read the test name and know where the buggy code is and how to reproduce the bug.</p><p>An easy and effective way to name your methods is to use these three questions:</p><ul><li>What you are testing?</li><li>What are the parameters of the test?</li><li>What is the expected result of the test?</li></ul><p>For example, consider a scenario where you want to add a test for the <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge">Divide</code></strong></span> method:</p><p><strong>Java</strong></p><div class="code panel pdl conf-macro output-block" data-hasbody="true" data-macro-name="code" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Default" data-theme="Default">float Divide(float dividend, float divisor) {
  return dividend / divisor;
}

...

@Test
void &lt;--TestMethodName--&gt;() {
    assertThrows(Divide(10, 0))
}</pre>
</div></div><p>If you use a simple test name, such as <span style="letter-spacing: 0.0px;"><strong><code class="highlighter-rouge">testDivide()</code></strong></span>, you are missing important information such as the expected action, parameter information, and expected test result. As a result, triaging a test failure requires you to look at the test implementation to see what the test does.</p><p>Instead, use a name such as <span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge">invokingDivideWithDivisorEqualToZeroThrowsException()</code></strong></span>, which specifies:</p><ul><li>the expected action of the test (<span style="color: rgb(255,0,0);"><strong><code class="highlighter-rouge">invokingDivide</code></strong></span>)</li><li>details about important parameters (the divisor is zero)</li><li>the expected result (the test throws an exception)</li></ul><p>If this test fails, you can look at the descriptive test name to find the most probable cause of the failure. In addition, test frameworks and test result dashboards use the test name when reporting test results. Descriptive names enable contributors to look at test suite results and easily see what features are failing.</p><p>Long method names are not a problem for test code. Test names are rarely used (usually when you triage and debug), and when you do need to look at a test, it is helpful to have descriptive names.</p><h3 id="ContributionTestingGuide-Useapre-committestifpossible">Use a pre-commit test if possible</h3><p>Post-commit tests validate that Beam works correctly in broad variety of scenarios. The tests catch errors that are hard to predict in the design and implementation stages</p><p>However, we often write a test to verify a specific scenario. In this situation, it is usually possible to implement the test as a unit test or a component test. You can add your unit tests or component tests to the pre-commit test suite, and the pre-commit test results give you faster code health feedback during the development stage, when a bug is cheap to fix.</p><p><br/></p><p><br/></p><p><br/></p>