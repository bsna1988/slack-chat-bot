<p>This page includes tips and troubleshooting information regarding Apache Beam's multi-language pipelines framework. For full documentation on multi-language pipelines, please see <a class="external-link" href="https://beam.apache.org/documentation/programming-guide/#multi-language-pipelines" rel="nofollow">here</a>.</p><p><div class="toc-macro client-side-toc-macro  conf-macro output-block" data-hasbody="false" data-headerelements="H1,H2,H3,H4,H5,H6,H7" data-macro-name="toc"> </div></p><h1 id="MultilanguagePipelinesTips-&quot;java:commandnotfound&quot;whenstartingthepipeline"><span style="color: rgb(0,0,0);">&quot;java: command not found&quot; when starting the pipeline</span></h1><p><span style="color: rgb(0,0,0);">This usually occurs due to <em>java</em> command not being available when submitting a multi-language pipeline that uses a Java transform to the Beam runner. Multi-language wrappers implemented in the pipeline SDK may try to automatically start up a Java expansion service, hence <em>java</em> command being available in the system is a pre-requisite. This can be resolved by installing JDK in the machine where the job is submitted from and adding the JDK directory with the <em>java</em> binary to the environment variable PATH.</span></p><h1 id="MultilanguagePipelinesTips-grpcerror&quot;failedtoconnecttoalladdresses&quot;whensubmittingthejob"><span style="color: rgb(0,0,0);">grpc error &quot;failed to connect to all addresses&quot; when submitting the job</span></h1><p><span style="color: rgb(0,0,0);">This usually occurs when an expansion service that is used by the pipeline is not available. For example, it could be that you simply forgot to start the expansion service before running the job. Or it could be that a Java expansion service that is automatically started up by a wrapper implemented in the pipeline SDK failed for some reason.</span></p><h1 id="MultilanguagePipelinesTips-&quot;KeyError:&#39;beam:coders:javasdk:0.1&#39;&quot;whenexpandingthetransformusingexpansionservice"><span style="color: rgb(0,0,0);">&quot;KeyError: 'beam:coders:javasdk:0.1'&quot; when expanding the transform using expansion service</span></h1><p>This occurs due to Java expansion service returning an expanded transform that uses a Java specific coder as one of its outputs. Cross-language transforms require coders used at the SDK boundary to be<span> </span> <a class="external-link" href="https://github.com/apache/beam/blob/05428866cdbf1ea8e4c1789dd40327673fd39451/model/pipeline/src/main/proto/beam_runner_api.proto#L784" rel="nofollow">Beam Standard Coders</a> <span> </span>that can be interpreted by all SDKs. Note that internal sub-transforms of the expanded transforms may choose Java specify coders. What matters are final outputs produced by the expanded Java transform.</p><p>The solution will be to update the user's transform to produce output <em>PCollection</em> types that use standard coders at the SDK boundaries.</p><h1 id="MultilanguagePipelinesTips-&quot;java.lang.IllegalArgumentException:UnknownCoderURNbeam:coder:pickled_python:v1&quot;whenrunneraPythonpipeline"><span style="color: rgb(0,0,0);">&quot;java.lang.IllegalArgumentException: Unknown Coder URN beam:coder:pickled_python:v1&quot; when runner a Python pipeline</span></h1><p><span style="color: rgb(0,0,0);">This usually means that an expansion request that is sent from Python SDK to a Java expansion service contained Python specific <em>PickleCoder</em> that Java SDK cannot interpret. For example, this could be due to following.</span></p><ul><li><span style="color: rgb(0,0,0);">Input <em>PCollection</em> that is fed into the cross-language transform in Python side uses a Python specific type. In this case, the <em>PTranform</em> that produced the input <em>PCollection</em> has to be updated to produce outputs that use<span> </span> <a class="external-link" href="https://github.com/apache/beam/blob/05428866cdbf1ea8e4c1789dd40327673fd39451/model/pipeline/src/main/proto/beam_runner_api.proto#L784" rel="nofollow">Beam's Standard Coders</a> <span> </span>to make sure Java side can interpret such coders.</span></li><li><span style="color: rgb(0,0,0);">Input <em>PCollection</em> uses standard coders but Python type inferencing results in picking up the <em>PickleCoder</em>. This can usually be resolved by annotating the predecessor Python transform with the correct type annotation using the <em>with_output_types</em> tag. See<span> </span> <a class="external-link" href="https://issues.apache.org/jira/browse/BEAM-11938" rel="nofollow">this Jira</a> <span> </span>for an example.</span></li></ul><p><br/></p><h1 id="MultilanguagePipelinesTips-&quot;UnknownCoderURNbeam:coder:pickled_python:v1&quot;whenrunningaJavapipelinethatusesPythoncross-languagetransforms"><span style="color: rgb(0,0,0);">&quot;Unknown Coder URN beam:coder:pickled_python:v1&quot; when running a Java pipeline that uses Python cross-language transforms</span></h1><p><span style="color: rgb(0,0,0);">This usually means that Python SDK was not able to properly determine a portable output <em>PCollection</em> type when expanding the cross-language transform. So it ended up picking the default <em>PickleCoder</em>.</span></p><p><span style="color: rgb(0,0,0);">But Java SDK is unable to interpret this, so it will fail when trying to parse the expansion response from the Python SDK.</span></p><p><span style="color: rgb(0,0,0);">The solution is to provide a hint to the Python SDK regarding the element type(s) of the output <em>PCollection</em>(s) of the cross-language transform. This can be provided using the <em> <a class="external-link" href="https://github.com/apache/beam/blob/cd5f88abbb571618b633b6a0068ef4f14fac55d3/sdks/java/extensions/python/src/main/java/org/apache/beam/sdk/extensions/python/PythonExternalTransform.java#L259" rel="nofollow">withOutputCoder</a> </em> or <em> <a class="external-link" href="https://github.com/apache/beam/blob/cd5f88abbb571618b633b6a0068ef4f14fac55d3/sdks/java/extensions/python/src/main/java/org/apache/beam/sdk/extensions/python/PythonExternalTransform.java#L243" rel="nofollow">withOutputCoders</a> </em> methods of the <em> <a class="external-link" href="https://github.com/apache/beam/blob/master/sdks/java/extensions/python/src/main/java/org/apache/beam/sdk/extensions/python/PythonExternalTransform.java" rel="nofollow">PythonExternalTransform</a> </em> API.</span></p><p><br/></p><h1 id="MultilanguagePipelinesTips-HowtosetJavaloglevelfromaPythonpipelinethatusesJavatransforms"><span style="color: rgb(0,0,0);">How to set Java log level from a Python pipeline that uses Java transforms</span></h1><p><span style="color: rgb(0,0,0);">For supported runners (e.g. portable runners and Dataflow runner), you can set the log level of Java transforms in the same way of setting python module log level overrides, specifically, using the <span style="color: rgb(32,33,36);">--sdk_harness_log_level_overrides</span> pipeline option. The python_underline_style option names will be automatically translated to Java smallCamel style and recognized by the Java SDK harness.</span></p><p><span style="color: rgb(0,0,0);">If the runner does not support the automatic mapping of options, One can try adding the corresponding pipeline option as a local pipeline option explicitly in Python side. For example, to suppress all logs from Java <em> <span style="color: rgb(32,33,36);">org.apache.kafka</span> </em> package you can do following.</span></p><ol><li><span style="color: rgb(0,0,0);">Add a Python PipelineOption that represents the corresponding Java PipelineOption available <a class="external-link" href="https://github.com/apache/beam/blob/a75ccc8d30a9d39ce2e92dc74189df8da9a4b240/sdks/java/core/src/main/java/org/apache/beam/sdk/options/SdkHarnessOptions.java#L84" rel="nofollow">here</a>. This can be simply added to your Python program that starts up the Beam job.</span><div class="code panel pdl conf-macro output-block" data-hasbody="true" data-macro-name="code" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: py; gutter: false; theme: Default" data-theme="Default">class JavaLoggingOptions(PipelineOptions):
  @classmethod
  def _add_argparse_args(cls, parser):
    parser.add_argument(
        '--sdkHarnessLogLevelOverrides',
        default={},
        type=json.loads,
        help=(
          'Java log level overrides'))</pre>
</div></div></li><li><p>Specify the additional PipelineOption as a parameter when running the Beam pipeline.</p><div class="code panel pdl conf-macro output-block" data-hasbody="true" data-macro-name="code" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: bash; gutter: false; theme: Default" data-theme="Default">--sdkHarnessLogLevelOverrides &quot;{\&quot;org.apache.kafka\&quot;:\&quot;ERROR\&quot;}&quot;</pre>
</div></div></li></ol><h1 id="MultilanguagePipelinesTips-DebuggingaPythonTestthatcallsaJavatransform">Debugging a Python Test that calls a Java transform</h1><p>The public <a class="external-link" href="https://beam.apache.org/documentation/programming-guide/#multi-language-pipelines" rel="nofollow">documentation</a> is great on creating and using cross-language transforms. For developers, however, running tests and debugging them is a bit different.  </p><p>We will showcase how to debug the following test <code>sql_test.SqlTransformTest.test_two_pcoll_same_schema.</code></p><h2 id="MultilanguagePipelinesTips-SetuptheinputargumentstothePythontest.">Set up the input arguments to the Python test.</h2><p>Beside the test function, right click the play button, and click &quot;Modify Run Configuration...&quot;</p><p><span class="confluence-embedded-file-wrapper"><img class="confluence-embedded-image" draggable="false" src="/confluence/download/attachments/199533334/Screen%20Shot%202022-02-17%20at%206.10.27%20PM.png?version=1&amp;modificationDate=1645143039000&amp;api=v2" data-image-src="/confluence/download/attachments/199533334/Screen%20Shot%202022-02-17%20at%206.10.27%20PM.png?version=1&amp;modificationDate=1645143039000&amp;api=v2" data-unresolved-comment-count="0" data-linked-resource-id="199535436" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="Screen Shot 2022-02-17 at 6.10.27 PM.png" data-base-url="https://cwiki.apache.org/confluence" data-linked-resource-content-type="image/png" data-linked-resource-container-id="199533334" data-linked-resource-container-version="9" alt=""></span> <span style="letter-spacing: 0.0px;"> </span></p><p>In <code style="letter-spacing: 0.0px;">Additional Arguments</code> <span style="letter-spacing: 0.0px;"> , add the arguments. For this example, we add </span></p><div class="code panel pdl conf-macro output-block" data-hasbody="true" data-macro-name="code" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Default" data-theme="Default">--test-pipeline-options=&quot;--runner=FlinkRunner --beam_services='{\&quot;:sdks:java:extensions:sql:expansion-service:shadowJar\&quot;: \&quot;localhost:8097\&quot;}'&quot;</pre>
</div></div><p><span class="confluence-embedded-file-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image" draggable="false" height="250" src="/confluence/download/attachments/199533334/Screen%20Shot%202022-02-17%20at%206.11.20%20PM.png?version=1&amp;modificationDate=1645143096000&amp;api=v2" data-image-src="/confluence/download/attachments/199533334/Screen%20Shot%202022-02-17%20at%206.11.20%20PM.png?version=1&amp;modificationDate=1645143096000&amp;api=v2" data-unresolved-comment-count="0" data-linked-resource-id="199535437" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="Screen Shot 2022-02-17 at 6.11.20 PM.png" data-base-url="https://cwiki.apache.org/confluence" data-linked-resource-content-type="image/png" data-linked-resource-container-id="199533334" data-linked-resource-container-version="9" alt=""></span></p><h2 id="MultilanguagePipelinesTips-RuntheJavaexpansionservice"><span>Run the Java expansion service</span></h2><p><span>The Gradle build <a class="external-link" href="https://github.com/apache/beam/blob/master/sdks/java/extensions/sql/expansion-service/build.gradle" rel="nofollow">file</a> defines how to run the expansion service. To do this manually, go to the Gradle tab in Intellij, go to this path <code>beam/sdks/java/extensions/sql/expanstion-service/tasks/other/</code> </span> <span>, and find the `</span>runExpansionService` entry.</p><ul><li>To run the expansion service without debugging, right click <code>runExpansionService</code> , and simply click &quot;run&quot;.</li><li>If you want to debug the Java code, set breakpoints in the IDE in Java files that are part of the expansion service. (i.e. ExpansionService.Java). Then click &quot;Debug&quot;</li></ul><p><span class="confluence-embedded-file-wrapper"><img class="confluence-embedded-image" draggable="false" src="/confluence/download/attachments/199533334/Screen%20Shot%202022-02-17%20at%206.09.33%20PM.png?version=1&amp;modificationDate=1645142991000&amp;api=v2" data-image-src="/confluence/download/attachments/199533334/Screen%20Shot%202022-02-17%20at%206.09.33%20PM.png?version=1&amp;modificationDate=1645142991000&amp;api=v2" data-unresolved-comment-count="0" data-linked-resource-id="199535435" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="Screen Shot 2022-02-17 at 6.09.33 PM.png" data-base-url="https://cwiki.apache.org/confluence" data-linked-resource-content-type="image/png" data-linked-resource-container-id="199533334" data-linked-resource-container-version="9" alt=""></span></p><h2 id="MultilanguagePipelinesTips-RunthePythontest">Run the Python test</h2><ul><li>If you don't want to debug the Python test, right click the button next to the test, and click <code>Run 'pytest for sql_test...'</code> </li><li>If you do want to debug the Python test, first set the breakpoints in your code. Then right click the button next to the test, and click <code>'Debug 'pytest for sql_test...'</code> </li></ul><h2 id="MultilanguagePipelinesTips-Notes">Notes</h2><p>You can choose to run only Python in debug mode and the expansion service not in debug mode; Java expansion service in debug mode and the Python code not in debug mode; both in debug mode; or neither in debug mode.</p><h1 id="MultilanguagePipelinesTips-Runningacross-languagetransformthatusesadifferentversionofanexternaltransform">Running a cross-language transform that uses a different version of an external transform</h1><p>By default, cross-language transforms released with Beam will automatically startup an expansion service that includes external transforms. Usually these transforms will be from the same Beam release as the pipeline SDK. For example, when using cross-language Kafka transforms from Python SDK, underlying Java KafkaIO transforms will be from the same released SDK version. If you need to use a different external SDK version you can do the following.</p><ul><li>Startup an expansion service that includes external transforms from a compatible SDK version. See <a class="external-link" href="https://beam.apache.org/documentation/sdks/python-multi-language-pipelines/#choose-an-expansion-service" rel="nofollow">here</a> for more details.</li><li>Specify the expansion service when defining the cross-language transform in your pipeline. For example, expansion service used by Python ReadFromKafka transform can be overridden <a class="external-link" href="https://github.com/apache/beam/blob/5b0e92f691486c034d6d4f4b315d3a7a7fe66cf5/sdks/python/apache_beam/io/kafka.py#L137" rel="nofollow">here</a>.</li></ul><p><br/></p><p><br/></p><p><br/></p>